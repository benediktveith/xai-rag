{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02417573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import shap\n",
    "import numpy as np\n",
    "from src.modules.data_loader import DataLoader\n",
    "from src.modules.llm_client import LLMClient\n",
    "from src.modules.rag_engine import RAGEngine\n",
    "from src.modules.multihop_rag_engine import MultiHopRAGEngine\n",
    "from src.modules.shap_explainer import ShapExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10ca158",
   "metadata": {},
   "source": [
    "# DeepShap for NRM's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d57a2f",
   "metadata": {},
   "source": [
    "Here we will essentially try to rebuild the DeepShap Explanantion Algorithm for NRM's (Neural Retrieval Models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7427269f",
   "metadata": {},
   "source": [
    "The different aspect, compared to the other Posthoc Methods we discuss, is that here, we will try to explain the ranking of the documents themselves. In contrast to explaining where the information among the selected documents came from. This is a key difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bd3b32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a2cd6a4",
   "metadata": {},
   "source": [
    "## Evaluation Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac16393",
   "metadata": {},
   "source": [
    "**NOTE**: We are using the FullWiki version of the HotPotQA Dataset. Meaning the **entire Wiki** is the context for each question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5826c3",
   "metadata": {},
   "source": [
    "There are no ground truth explanations available for any neural model. We therefore have to use different evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cc3e86",
   "metadata": {},
   "source": [
    "1. In the Paper \"A study on the Interpretability of Neural Retrieval Models using\n",
    "DeepSHAP\" a LIME based explanation was used as proxy (https://github.com/marcotcr/lime)\n",
    "2. **Faithfullness/Fidelity** We can also use a perturbation based evaluation approach. Meaning we will leave out the as important identified tokens in the query and rerun the retrieval comparing the ranking to each other (AOPC = \"Area over perturbation curve\"). (https://github.com/CristianCosci/AOPC_MoRF)\n",
    "3. **Sparseness** Leave out all tokens except the top k identified by DeepSHAP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d2556c",
   "metadata": {},
   "source": [
    "${AOPC}_M = \\frac{1}{L+1} \\left\\langle \\sum_{k=1}^{L} f(x^{(0)}_M) - f(x^{(k)}_M) \\right\\rangle_{p(X)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf1b663",
   "metadata": {},
   "source": [
    "### Experiment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd78e2d",
   "metadata": {},
   "source": [
    "Bound by the Computational cost of DeepShap! (Run on KIGS Server?)\n",
    "1. Select a finite amount of instances from the HotpotQA Dataset.\n",
    "2. Conduct the Retrieval on those instances.\n",
    "3. Compute Metrics for every Instance and average them to get overall score for DeepShap Explanability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd193f0",
   "metadata": {},
   "source": [
    "It is crucial to select the fitting Background image for DeepSHAP. Fortunatly this research was already conducted by [Fernando et. al.](https://arxiv.org/abs/1907.06484). The selection is dependent on the NRM used for the retrieval, tho performances are fairly similar for each NRM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9891186",
   "metadata": {},
   "source": [
    "![Performance Metrics Background Images](img/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5814cb",
   "metadata": {},
   "source": [
    "**ATTENTION**: We currently use \"sentence-transformers/all-MiniLM-L6-v2\" as the embedding model. This is a simple Bi-Encoder and no NRM. A NRM is a Reranker, using both query and document together to generate a relevance score instead of an embedding vector. These models yield significantly better matching results for query and documents, but to the drawback of computational cost and time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df313e3e",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Discuss for general Experimental setup.\n",
    "\n",
    "1. Explaining the output of the LLM based on the retrieved documents.\n",
    "2. Explaining the retrieved documents.\n",
    "\n",
    "This leads to key questions.\n",
    "\n",
    "1. Should we compare those two experiments or link them in some way (**A sort of Pipeline for Explaining Retrieval (so both sides: Retrieval and LLM)**)\n",
    "- If so how?\n",
    "2. Should the setups for the experiments differ? \n",
    "- Using a NRM Reranking for DeepShap and no NRM for other project part?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b0c584",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4287b14",
   "metadata": {},
   "source": [
    "First test implementation of DeepShap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59237bd",
   "metadata": {},
   "source": [
    "**NOTES:**\n",
    "- We are looking at only the top document and calculate the AOPC-Metric for only that document. Since it was ranked the highest by the Retriever (no matter if it's a NRM or just Similarity Measure) we can assume that the identified Tokens are relevant for the Query.#\n",
    "- We can also use LIME for Explanation or Comparison for DeepShap, the implementation is straight forward in fairly similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0fd819",
   "metadata": {},
   "source": [
    "### 1. Shap for Bi-Encoder using shap.KernelExplainer (Cosine Similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf07c1d",
   "metadata": {},
   "source": [
    "The **KernelExplainer** is similar to LIME, cause it explains a local instance using the set of locally pertubated instances and fits a linear model on the set. It differs by the way it approximates the values if that model, it uses the approximation of shapely-values, whereas LIME uses a simple exponential kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebfae2c",
   "metadata": {},
   "source": [
    "We use the top Document for the AOPC Metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ab46bd",
   "metadata": {},
   "source": [
    "Setup RAG-Engine with LLM-Client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4ff076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new vector store with 9427 documents...\n",
      "Split into 15533 chunks.\n",
      "RagEngine ready.\n"
     ]
    }
   ],
   "source": [
    "dataLoader = DataLoader()\n",
    "documents = DataLoader.setup()\n",
    "\n",
    "ragEngine = RAGEngine()\n",
    "RAGEngine.setup(documents=documents)\n",
    "\n",
    "client = LLMClient(provider=\"ollama\", model_name=\"gemma3:1b\")\n",
    "mh_rag = MultiHopRAGEngine(rag_engine=ragEngine, llm_client=client, num_hops=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58301517",
   "metadata": {},
   "source": [
    "Setup and run Multi-Hop Retrieval experiment. For this we will select a random set of questions from the dataset()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b269cc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# select question randomly (with seed)\u001b[39;00m\n\u001b[32m      2\u001b[39m N = \u001b[32m5\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m rng = \u001b[43mnp\u001b[49m.random.default_rng(seed=\u001b[32m78\u001b[39m)\n\u001b[32m      5\u001b[39m selected_documents = rng.choice(documents, N)\n\u001b[32m      7\u001b[39m \u001b[33;03m\"\"\"doc = Document(\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03m                    page_content=content,\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m                    metadata={\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m \u001b[33;03m                    }\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[33;03m                )\"\"\"\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# select question randomly (with seed)\n",
    "N = 5\n",
    "rng = np.random.default_rng(seed=78)\n",
    "\n",
    "selected_documents = rng.choice(documents, N)\n",
    "\n",
    "\"\"\"doc = Document(\n",
    "                    page_content=content,\n",
    "                    metadata={\n",
    "                        \"title\": title,\n",
    "                        \"question_id\": question_id,\n",
    "                        \"question\": question,\n",
    "                        \"answer\": answer,\n",
    "                        # Crucial for Plausibility Metric:\n",
    "                        \"is_supporting\": is_supporting, \n",
    "                        \"gold_sentence_indices\": \",\".join(map(str, gold_indices)), \n",
    "                        \"source\": \"hotpotqa\"\n",
    "                    }\n",
    "                )\"\"\"\n",
    "\n",
    "# storing all traces\n",
    "traces = []\n",
    "\n",
    "for doc in selected_documents:\n",
    "    question = doc.metadata.get(\"question\")\n",
    "\n",
    "    trace, all_documents = mh_rag.run_and_trace(question)\n",
    "\n",
    "    traces.append(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ef9930",
   "metadata": {},
   "source": [
    "Setup Explainer and explain retrievals for every hop. What else could be explained here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8c3c4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 5 (1305517509.py, line 8)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mexplanations = []\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after 'for' statement on line 5\n"
     ]
    }
   ],
   "source": [
    "# set mode to bi-encoder, meaning we won't explain a NRM\n",
    "explainer = ShapExplainer(mode=\"bi-encoder\")\n",
    "explanations = []\n",
    "\n",
    "for trace in traces: \n",
    "\n",
    "    explanation = {\"explanation_hops\": explainer.explain(trace), \n",
    "                   \"initial_query\": trace[\"initial_query\"],\n",
    "                   \"final_answer\": trace[\"final_answer\"]}\n",
    "    \n",
    "    explanations.append(explanation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a425a6d0",
   "metadata": {},
   "source": [
    "### 2. LIME Explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53752eb2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42b06674",
   "metadata": {},
   "source": [
    "### 2. DeepShap for NRM's (cross-encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1885f344",
   "metadata": {},
   "source": [
    "More powerful and insightful explanations. Also this is closer to the \"real world\" RAG System, since powerful systems use NRM's for ranking. (**Explain NRM shortly**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1820ad53",
   "metadata": {},
   "source": [
    "Here we would need to use shap.DeepExplainer() designed for Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2c7be0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae57bea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
