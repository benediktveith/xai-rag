{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02417573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import shap\n",
    "import numpy as np\n",
    "from src.modules.data_loader_single_hop import BoolQDataLoader\n",
    "from src.modules.llm_client import LLMClient\n",
    "from src.modules.rag_engine import RAGEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10ca158",
   "metadata": {},
   "source": [
    "# DeepShap for NRM's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d57a2f",
   "metadata": {},
   "source": [
    "Here we will essentially try to rebuild the DeepShap Explanantion Algorithm for NRM's (Neural Retrieval Models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7427269f",
   "metadata": {},
   "source": [
    "The different aspect, compared to the other Posthoc Methods we discuss, is that here, we will try to explain the ranking of the documents themselves. In contrast to explaining where the information among the selected documents came from. This is a key difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bd3b32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a2cd6a4",
   "metadata": {},
   "source": [
    "## Evaluation Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac16393",
   "metadata": {},
   "source": [
    "**NOTE**: We are using the FullWiki version of the HotPotQA Dataset. Meaning the **entire Wiki** is the context for each question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5826c3",
   "metadata": {},
   "source": [
    "There are no ground truth explanations available for any neural model. We therefore have to use different evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cc3e86",
   "metadata": {},
   "source": [
    "1. In the Paper \"A study on the Interpretability of Neural Retrieval Models using\n",
    "DeepSHAP\" a LIME based explanation was used as proxy (https://github.com/marcotcr/lime)\n",
    "2. **Faithfullness/Fidelity** We can also use a perturbation based evaluation approach. Meaning we will leave out the as important identified tokens in the query and rerun the retrieval comparing the ranking to each other (AOPC = \"Area over perturbation curve\"). (https://github.com/CristianCosci/AOPC_MoRF)\n",
    "3. **Sparseness** Leave out all tokens except the top k identified by DeepSHAP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d2556c",
   "metadata": {},
   "source": [
    "${AOPC}_M = \\frac{1}{L+1} \\left\\langle \\sum_{k=1}^{L} f(x^{(0)}_M) - f(x^{(k)}_M) \\right\\rangle_{p(X)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf1b663",
   "metadata": {},
   "source": [
    "### Experiment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd78e2d",
   "metadata": {},
   "source": [
    "Bound by the Computational cost of DeepShap! (Run on KIGS Server?)\n",
    "1. Select a finite amount of instances from the HotpotQA Dataset.\n",
    "2. Conduct the Retrieval on those instances.\n",
    "3. Compute Metrics for every Instance and average them to get overall score for DeepShap Explanability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd193f0",
   "metadata": {},
   "source": [
    "It is crucial to select the fitting Background image for DeepSHAP. Fortunatly this research was already conducted by [Fernando et. al.](https://arxiv.org/abs/1907.06484). The selection is dependent on the NRM used for the retrieval, tho performances are fairly similar for each NRM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9891186",
   "metadata": {},
   "source": [
    "![Performance Metrics Background Images](img/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5814cb",
   "metadata": {},
   "source": [
    "**ATTENTION**: We currently use \"sentence-transformers/all-MiniLM-L6-v2\" as the embedding model. This is a simple Bi-Encoder and no NRM. A NRM is a Reranker, using both query and document together to generate a relevance score instead of an embedding vector. These models yield significantly better matching results for query and documents, but to the drawback of computational cost and time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df313e3e",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Discuss for general Experimental setup.\n",
    "\n",
    "1. Explaining the output of the LLM based on the retrieved documents.\n",
    "2. Explaining the retrieved documents.\n",
    "\n",
    "This leads to key questions.\n",
    "\n",
    "1. Should we compare those two experiments or link them in some way (**A sort of Pipeline for Explaining Retrieval (so both sides: Retrieval and LLM)**)\n",
    "- If so how?\n",
    "2. Should the setups for the experiments differ? \n",
    "- Using a NRM Reranking for DeepShap and no NRM for other project part?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b0c584",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4287b14",
   "metadata": {},
   "source": [
    "First test implementation of DeepShap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59237bd",
   "metadata": {},
   "source": [
    "**NOTES:**\n",
    "- We are looking at only the top document and calculate the AOPC-Metric for only that document. Since it was ranked the highest by the Retriever (no matter if it's a NRM or just Similarity Measure) we can assume that the identified Tokens are relevant for the Query.#\n",
    "- We can also use LIME for Explanation or Comparison for DeepShap, the implementation is straight forward in fairly similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0fd819",
   "metadata": {},
   "source": [
    "**1. DeepShap for Bi-Encoder (Cosine Similarity)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebfae2c",
   "metadata": {},
   "source": [
    "We use the top Document for the AOPC Metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d3e5c8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d4ff076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new vector store with 9427 documents...\n",
      "Split into 15533 chunks.\n",
      "RagEngine ready.\n"
     ]
    }
   ],
   "source": [
    "DataLoader = BoolQDataLoader()\n",
    "documents = DataLoader.setup()\n",
    "\n",
    "RAGEngine = RAGEngine()\n",
    "RAGEngine.setup(documents=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b06674",
   "metadata": {},
   "source": [
    "**2. DeepShap for NRM's (cross-encoder)<br>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1885f344",
   "metadata": {},
   "source": [
    "More powerful and insightful explanations. Also this is closer to the \"real world\" RAG System, since powerful systems use NRM's for ranking. (**Explain NRM shortly**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2c7be0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae57bea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
