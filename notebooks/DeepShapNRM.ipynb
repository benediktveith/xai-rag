{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02417573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import shap\n",
    "import numpy as np\n",
    "from src.modules.data_loader import DataLoader\n",
    "from src.modules.llm_client import LLMClient\n",
    "from src.modules.rag_engine import RAGEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adfa2fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading HotPotQA from http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_test_fullwiki_v1.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 46.2M/46.2M [00:04<00:00, 9.40MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Download complete.\n",
      "Loading data into memory...\n",
      "✓ Loaded 7405 questions.\n",
      "Converting HotPotQA contexts to documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles: 100%|██████████| 7405/7405 [00:00<00:00, 7496.95it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 73774 context chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# setup data\n",
    "\n",
    "DataLoader = DataLoader()\n",
    "documents = DataLoader.setup()\n",
    "\n",
    "# setup RAG-Engine\n",
    "\n",
    "# RAGEngine = RAGEngine()\n",
    "# RAGEngine.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10ca158",
   "metadata": {},
   "source": [
    "# DeepShap for NRM's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d57a2f",
   "metadata": {},
   "source": [
    "Here we will essentially try to rebuild the DeepShap Explanantion Algorithm for NRM's (Neural Retrieval Models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7427269f",
   "metadata": {},
   "source": [
    "The different aspect, compared to the other Posthoc Methods we discuss, is that here, we will try to explain the ranking of the documents themselves. In contrast to explaining where the information among the selected documents came from. This is a key difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bd3b32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a2cd6a4",
   "metadata": {},
   "source": [
    "## Evaluation Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5826c3",
   "metadata": {},
   "source": [
    "There are no ground truth explanations available for any neural model. We therefore have to use different evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cc3e86",
   "metadata": {},
   "source": [
    "1. In the Paper \"A study on the Interpretability of Neural Retrieval Models using\n",
    "DeepSHAP\" a LIME based explanation was used as proxy\n",
    "2. **Faithfullness/Fidelity** We can also use a perturbation based evaluation approach. Meaning we will leave out the as important identified tokens in the query and rerun the retrieval comparing the ranking to each other (AOPC = \"Area over perturbation curve\"). (https://github.com/CristianCosci/AOPC_MoRF)\n",
    "3. **Sparseness** Leave out all tokens except the top k identified by DeepSHAP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f4708",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai-rag (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
