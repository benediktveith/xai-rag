{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02417573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import shap\n",
    "import numpy as np\n",
    "from src.modules.data_loader import DataLoader\n",
    "from src.modules.llm_client import LLMClient\n",
    "from src.modules.rag_engine import RAGEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adfa2fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading HotPotQA from http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_test_fullwiki_v1.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 46.2M/46.2M [00:04<00:00, 9.40MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Download complete.\n",
      "Loading data into memory...\n",
      "✓ Loaded 7405 questions.\n",
      "Converting HotPotQA contexts to documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles: 100%|██████████| 7405/7405 [00:00<00:00, 7496.95it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 73774 context chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# setup data\n",
    "\n",
    "DataLoader = DataLoader()\n",
    "documents = DataLoader.setup()\n",
    "\n",
    "# setup RAG-Engine\n",
    "\n",
    "# RAGEngine = RAGEngine()\n",
    "# RAGEngine.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10ca158",
   "metadata": {},
   "source": [
    "# DeepShap for NRM's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d57a2f",
   "metadata": {},
   "source": [
    "Here we will essentially try to rebuild the DeepShap Explanantion Algorithm for NRM's (Neural Retrieval Models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7427269f",
   "metadata": {},
   "source": [
    "The different aspect, compared to the other Posthoc Methods we discuss, is that here, we will try to explain the ranking of the documents themselves. In contrast to explaining where the information among the selected documents came from. This is a key difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bd3b32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a2cd6a4",
   "metadata": {},
   "source": [
    "## Evaluation Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac16393",
   "metadata": {},
   "source": [
    "**NOTE**: We are using the FullWiki version of the HotPotQA Dataset. Meaning the **entire Wiki** is the context for each question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5826c3",
   "metadata": {},
   "source": [
    "There are no ground truth explanations available for any neural model. We therefore have to use different evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cc3e86",
   "metadata": {},
   "source": [
    "1. In the Paper \"A study on the Interpretability of Neural Retrieval Models using\n",
    "DeepSHAP\" a LIME based explanation was used as proxy\n",
    "2. **Faithfullness/Fidelity** We can also use a perturbation based evaluation approach. Meaning we will leave out the as important identified tokens in the query and rerun the retrieval comparing the ranking to each other (AOPC = \"Area over perturbation curve\"). (https://github.com/CristianCosci/AOPC_MoRF)\n",
    "3. **Sparseness** Leave out all tokens except the top k identified by DeepSHAP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f4708",
   "metadata": {},
   "source": [
    "Make a Differentiation between Shap-Values for Document-Tokens and Shap-Values for Query-Tokens!\n",
    "1. We should probably focus on the Documents-Tokens, identifying why a document was selected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf1b663",
   "metadata": {},
   "source": [
    "### Experiment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd78e2d",
   "metadata": {},
   "source": [
    "Bound by the Computational cost of DeepShap! (Run on KIGS Server?)\n",
    "1. Select a finite amount of instances from the HotpotQA Dataset.\n",
    "2. Conduct the Retrieval on those instances.\n",
    "3. Compute Metrics for every Instance and average them to get overall score for DeepShap Explanability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd193f0",
   "metadata": {},
   "source": [
    "It is crucial to select the fitting Background image for DeepSHAP. Fortunatly this research was already conducted by [Fernando et. al.](https://arxiv.org/abs/1907.06484). The selection is dependent on the NRM used for the retrieval, tho performances are fairly similar for each NRM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9891186",
   "metadata": {},
   "source": [
    "![Performance Metrics Background Images](img/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5814cb",
   "metadata": {},
   "source": [
    "**ATTENTION**: We currently use \"sentence-transformers/all-MiniLM-L6-v2\" as the embedding model. This is a simple Bi-Encoder and no NRM. A NRM is a Reranker, using both query and document together to generate a relevance score instead of an embedding vector. These models yield significantly better matching results for query and documents, but to the drawback of computational cost and time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df313e3e",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Discuss for general Experimental setup.\n",
    "\n",
    "1. Explaining the output of the LLM based on the retrieved documents.\n",
    "2. Explaining the retrieved documents.\n",
    "\n",
    "This leads to key questions.\n",
    "\n",
    "1. Should we compare those two experiments or link them in some way (**A sort of Pipeline for Explaining Retrieval (so both sides: Retrieval and LLM)**)\n",
    "- If so how?\n",
    "2. Should the setups for the experiments differ? \n",
    "- Using a NRM Reranking for DeepShap and no NRM for other project part?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai-rag (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
