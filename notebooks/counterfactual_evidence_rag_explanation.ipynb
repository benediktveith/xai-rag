{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "601f6861",
   "metadata": {},
   "source": [
    "# Counterfactual Evidence-Based Explanation for RAG (Updated)\n",
    "\n",
    "This notebook implements a post-hoc explanation method for retrieval-augmented generation (RAG) systems. It focuses on **evidence support** rather than simple semantic similarity. Given a question and a set of retrieved documents, the RAG engine answers the question and then highlights **which parts of the retrieved documents directly support the answer**.\n",
    "\n",
    "This version corrects earlier issues by working with sentence indices instead of raw text when selecting evidence, ensuring proper mapping from document chunks back to sentences. It also skips evaluation for questions where no supportive evidence can be identified, preventing meaningless metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcb76d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\Admin\\Desktop\\XAI\\FINAL\\xai-rag\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up environment and paths\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "project_root = None\n",
    "for p in [NOTEBOOK_DIR] + list(NOTEBOOK_DIR.parents):\n",
    "    if (p / 'src').exists():\n",
    "        project_root = p\n",
    "        break\n",
    "if project_root is None:\n",
    "    raise RuntimeError('Notebook must live inside the repo')\n",
    "\n",
    "sys.path.insert(0, str(project_root))\n",
    "print('Project root:', project_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eacbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedMCQA questions: 59\n",
      "KG-capable questions: 12\n",
      "RAG hops: 2\n",
      "LLM config: {'provider': 'ollama', 'model': 'gemma3:4b'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load configuration settings\n",
    "import tomllib\n",
    "config_path = project_root / 'config.toml'\n",
    "with open(config_path, 'rb') as f:\n",
    "    cfg = tomllib.load(f)\n",
    "\n",
    "med_cfg = cfg['medmcqa']\n",
    "rag_cfg = cfg['rag']\n",
    "llm_cfg = cfg['llm']\n",
    "\n",
    "QUESTION_IDS = med_cfg['question_ids']\n",
    "KG_CAPABLE = set(med_cfg.get('kg_capable', []))\n",
    "SPLIT = med_cfg['split']\n",
    "\n",
    "print('MedMCQA questions:', len(QUESTION_IDS))\n",
    "print('KG-capable questions:', len(KG_CAPABLE))\n",
    "print('RAG hops:', rag_cfg['n_hops'])\n",
    "print('LLM config:', llm_cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fbf6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded questions: 59\n",
      "Question: Which of the following agents is likely to cause cerebral calcification and hydrocephalus in a newborn whose mother has history of taking spiramycin but was not compliant with therapy?\n",
      "Answer letter: B\n",
      "KG-capable: False\n",
      "Question: Myocarditis is caused bya) Pertussisb) Measlesc) Diptheriad) Scorpion sting\n",
      "Answer letter: A\n",
      "KG-capable: False\n",
      "Question: Childhood osteopetrosis is characterized by – a) B/L frontal bossingb) Multiple # (fracture)c) Hepatosplenomegalyd) Cataracte) Mental retardation\n",
      "Answer letter: A\n",
      "KG-capable: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load MedMCQA questions for evaluation\n",
    "from src.modules.loader.medmcqa_data_loader import MedMCQADataLoader\n",
    "\n",
    "loader = MedMCQADataLoader()\n",
    "documents = loader.setup()\n",
    "print('Loaded questions:', len(documents))\n",
    "\n",
    "# Display a few examples\n",
    "for d in documents[:3]:\n",
    "    print('Question:', d.metadata['question'])\n",
    "    print('Answer letter:', d.metadata['answer'])\n",
    "    print('KG-capable:', d.metadata['question_id'] in KG_CAPABLE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52309b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to local Ollama (gemma3:4b)...\n",
      "LLM ready: model='gemma3:4b' reasoning=False temperature=0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the LLM\n",
    "from src.modules.llm.llm_client import LLMClient\n",
    "\n",
    "llm_client = LLMClient(provider=llm_cfg['provider'], model_name=llm_cfg['model'])\n",
    "llm = llm_client.get_llm()\n",
    "print('LLM ready:', llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90753909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StatPearls build stats: StatPearlsBuildStats(tarball_downloaded=False, extracted=False, nxml_files_found=9629, jsonl_files_created=0, articles_loaded=300, chunks_emitted=13922)\n",
      "StatPearls chunks loaded: 13922\n",
      "Sample StatPearls metadata keys: dict_keys(['source', 'split', 'title', 'topic_name', 'source_filename', 'chunk_index', 'chunk_id'])\n",
      "Loading existing vector store from ../data/vector_db_statpearls...\n",
      "RagEngine ready.\n",
      "RAG engine initialized\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load StatPearls corpus and set up RAG engine\n",
    "from src.modules.loader.statspearls_data_loader import StatPearlsDataLoader\n",
    "from src.modules.rag.rag_engine import RAGEngine\n",
    "\n",
    "sp_loader = StatPearlsDataLoader()\n",
    "sp_output = sp_loader.setup()\n",
    "\n",
    "# Unpack documents and stats\n",
    "if isinstance(sp_output, tuple):\n",
    "    statpearls_docs, sp_stats = sp_output\n",
    "    print('StatPearls build stats:', sp_stats)\n",
    "else:\n",
    "    statpearls_docs = sp_output\n",
    "\n",
    "# Flatten nested lists\n",
    "if len(statpearls_docs) > 0 and isinstance(statpearls_docs[0], list):\n",
    "    statpearls_docs = [doc for article in statpearls_docs for doc in article]\n",
    "\n",
    "print('StatPearls chunks loaded:', len(statpearls_docs))\n",
    "print('Sample StatPearls metadata keys:', statpearls_docs[0].metadata.keys())\n",
    "\n",
    "# Initialize RAG engine over StatPearls\n",
    "rag = RAGEngine(persist_dir='../data/vector_db_statpearls')\n",
    "rag.setup(documents=statpearls_docs, reset=False, k=rag_cfg['n_hops'])\n",
    "print('RAG engine initialized')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d6854c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 2 documents\n",
      "Doc 0:\n",
      "programs on a large scale that offer both maternal or neonatal screening to identify infection in mothers and infants. No vaccines are present to prevent infection, and no efficacious and safe therapies are available for the treatment of maternal or fetal CMV infection. [22] In some setups, gancyclo\n",
      "Doc 1:\n",
      "of infant head lag are linked to conditions causing neonatal/infantile hypotonia. These conditions constitute a differential diagnosis list to consider when assessing an infant with persistent or severe head lag. This includes chromosome disorders (ie, Prader Willi), Hypoxic-ischemic injuries, cereb\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Retrieve some documents for a sample question to inspect retrieval\n",
    "sample_question = documents[0].metadata['question']\n",
    "sample_docs = rag.retrieve_documents(sample_question)\n",
    "print('Retrieved', len(sample_docs), 'documents')\n",
    "for i, d in enumerate(sample_docs[:2]):\n",
    "    print(f'Doc {i}:')\n",
    "    print(d.page_content[:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df5485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper functions for splitting text and highlighting\n",
    "import re\n",
    "import html\n",
    "\n",
    "def split_sentences(text: str):\n",
    "    text = (text or '').strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    return [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n",
    "\n",
    "def highlight_html(text: str, snippets):\n",
    "    if not snippets:\n",
    "        return html.escape(text)\n",
    "    out = html.escape(text)\n",
    "    for s in snippets:\n",
    "        out = out.replace(html.escape(s), f\"<mark>{html.escape(s)}</mark>\")\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db004c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate candidate windows of sentences for each document\n",
    "\n",
    "def candidate_windows(docs, max_sents_per_doc: int, window_size: int = 3):\n",
    "    per_doc = {}\n",
    "    for i, d in enumerate(docs):\n",
    "        sents = split_sentences(d.page_content)[:max_sents_per_doc]\n",
    "        windows = []\n",
    "        for idx in range(len(sents)):\n",
    "            start = max(0, idx - (window_size - 1))\n",
    "            window = ' '.join(sents[start: idx+1])\n",
    "            windows.append({\n",
    "                'sentence': sents[idx],\n",
    "                'window': window,\n",
    "                'sent_idx': idx\n",
    "            })\n",
    "        per_doc[i] = windows\n",
    "    return per_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b710f384",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract claims from baseline answer, resolving MCQ letters to option text\n",
    "\n",
    "def extract_claims(answer: str, question_doc=None):\n",
    "    a = (answer or '').strip()\n",
    "    if not a or a.lower() == 'unknown':\n",
    "        return []\n",
    "    # If answer is descriptive\n",
    "    if len(a) > 2:\n",
    "        return [a]\n",
    "    # Resolve MC letters\n",
    "    if question_doc is not None and a.upper() in {'A','B','C','D'}:\n",
    "        raw = question_doc.metadata.get('cop_raw')\n",
    "        if isinstance(raw, str):\n",
    "            for line in raw.splitlines():\n",
    "                if line.strip().startswith(a.upper()):\n",
    "                    return [line.split(':', 1)[-1].strip()]\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77792d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support checker ready\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if a window supports the claim using the LLM\n",
    "\n",
    "def support_check(claim: str, window: str):\n",
    "    prompt = (\n",
    "        f'''You are verifying evidence support.\n",
    "        Claim:\n",
    "        {claim}\n",
    "        Evidence context:\n",
    "        {window}\n",
    "        Decide if the evidence context states or clearly implies that the claim is correct.\n",
    "        Reply with exactly one token: YES or NO.'''\n",
    "    )\n",
    "    out = llm.invoke(prompt).content.strip().upper()\n",
    "    return out.startswith('YES')\n",
    "\n",
    "print('Support checker ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717d6f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helpers for removing/keeping sentences and computing curve prefixes\n",
    "\n",
    "def remove_indices(indices, sentences):\n",
    "    ix = set(indices)\n",
    "    return ' '.join([s for i, s in enumerate(sentences) if i not in ix])\n",
    "\n",
    "def keep_indices(indices, sentences):\n",
    "    ix = set(indices)\n",
    "    return ' '.join([s for i, s in enumerate(sentences) if i in ix])\n",
    "\n",
    "def curve_prefixes(indices, steps):\n",
    "    if not indices:\n",
    "        return [set() for _ in range(steps)]\n",
    "    n = len(indices)\n",
    "    cuts = []\n",
    "    for k in range(1, steps+1):\n",
    "        m = min(n, max(1, (k*n + steps - 1)//steps))\n",
    "        cuts.append(set(indices[:m]))\n",
    "    return cuts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e345c9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation core ready.\n"
     ]
    }
   ],
   "source": [
    "# Run one counterfactual evaluation for a question\n",
    "\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "\n",
    "LLM_CALLS = 0\n",
    "LLM_TIME_S = 0.0\n",
    "\n",
    "def timed_invoke(prompt: str) -> str:\n",
    "    global LLM_CALLS, LLM_TIME_S\n",
    "    t0 = time.time()\n",
    "    out = llm.invoke(prompt).content.strip()\n",
    "    LLM_CALLS += 1\n",
    "    LLM_TIME_S += (time.time() - t0)\n",
    "    return out\n",
    "\n",
    "def norm_choice(x: str) -> str:\n",
    "    x = (x or \"\").strip().upper()\n",
    "    m = re.search(r\"\\b([ABCD])\\b\", x)\n",
    "    return m.group(1) if m else \"\"\n",
    "\n",
    "def answer_choice_from_context(question: str, context: str) -> str:\n",
    "    prompt = f'''\n",
    "You are answering a multiple-choice question using only the provided context.\n",
    "\n",
    "Return exactly one token: A or B or C or D.\n",
    "If the answer cannot be derived from the context, return exactly: UNKNOWN\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer:\n",
    "'''.strip()\n",
    "    out = timed_invoke(prompt)\n",
    "    c = norm_choice(out)\n",
    "    return c if c in {\"A\",\"B\",\"C\",\"D\"} else \"UNKNOWN\"\n",
    "\n",
    "def answer_text_from_context(question: str, context: str) -> str:\n",
    "    prompt = f'''\n",
    "Answer the question using only the provided context.\n",
    "\n",
    "Return a short noun phrase (not a letter choice).\n",
    "If the answer cannot be derived from the context, return exactly: UNKNOWN\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer:\n",
    "'''.strip()\n",
    "    out = timed_invoke(prompt).strip()\n",
    "    return out if out and out.upper() != \"UNKNOWN\" else \"UNKNOWN\"\n",
    "\n",
    "def curve_prefixes(indices, steps: int):\n",
    "    if not indices:\n",
    "        return [set() for _ in range(steps)]\n",
    "    n = len(indices)\n",
    "    cuts = []\n",
    "    for k in range(1, steps + 1):\n",
    "        m = min(n, max(1, math.ceil(k * n / steps)))\n",
    "        cuts.append(set(indices[:m]))\n",
    "    return cuts\n",
    "\n",
    "def run_one(\n",
    "    question_doc,\n",
    "    curve_steps: int = 6,\n",
    "    max_support_per_claim: int = 3,\n",
    "    max_sentences_per_doc: int = 25,\n",
    "):\n",
    "    question = question_doc.metadata[\"question\"]\n",
    "    gold_choice = norm_choice(str(question_doc.metadata.get(\"answer\", \"\")))\n",
    "\n",
    "    docs = rag.retrieve_documents(question)\n",
    "    context = \"\\n\\n\".join(d.page_content for d in docs if d.page_content)\n",
    "\n",
    "    pred_choice = answer_choice_from_context(question, context)\n",
    "    pred_text = answer_text_from_context(question, context)\n",
    "\n",
    "    claims = [] if pred_text == \"UNKNOWN\" else [pred_text]\n",
    "\n",
    "    doc_offsets = []\n",
    "    global_sents = []\n",
    "    for d in docs:\n",
    "        sents = split_sentences(d.page_content)\n",
    "        doc_offsets.append(len(global_sents))\n",
    "        global_sents.extend(sents)\n",
    "\n",
    "    cands = candidate_windows(docs, max_sents_per_doc=max_sentences_per_doc, window_size=3)\n",
    "\n",
    "    global_indices = set()\n",
    "    if claims:\n",
    "        for claim in claims:\n",
    "            found = 0\n",
    "            for doc_idx, items in cands.items():\n",
    "                for item in items:\n",
    "                    if support_check(claim, item[\"window\"]):\n",
    "                        global_idx = doc_offsets[doc_idx] + item[\"sent_idx\"]\n",
    "                        global_indices.add(global_idx)\n",
    "                        found += 1\n",
    "                        if found >= max_support_per_claim:\n",
    "                            break\n",
    "                if found >= max_support_per_claim:\n",
    "                    break\n",
    "\n",
    "    hl_indices = sorted(global_indices)\n",
    "\n",
    "    ctx_wo = remove_indices(hl_indices, global_sents)\n",
    "    ctx_only = keep_indices(hl_indices, global_sents)\n",
    "\n",
    "    pred_wo = answer_choice_from_context(question, ctx_wo) if ctx_wo.strip() else \"UNKNOWN\"\n",
    "    pred_only = answer_choice_from_context(question, ctx_only) if ctx_only.strip() else \"UNKNOWN\"\n",
    "\n",
    "    comprehensiveness = int(pred_wo != pred_choice)\n",
    "    sufficiency = int(pred_only == pred_choice)\n",
    "\n",
    "    prefixes = curve_prefixes(hl_indices, curve_steps)\n",
    "\n",
    "    del_curve = []\n",
    "    for pref in prefixes:\n",
    "        c = remove_indices(sorted(pref), global_sents)\n",
    "        a = answer_choice_from_context(question, c) if c.strip() else \"UNKNOWN\"\n",
    "        del_curve.append(int(a != pred_choice))\n",
    "    deletion_auc = sum(del_curve) / len(del_curve) if del_curve else 0.0\n",
    "\n",
    "    ins_curve = []\n",
    "    for pref in prefixes:\n",
    "        c = keep_indices(sorted(pref), global_sents)\n",
    "        a = answer_choice_from_context(question, c) if c.strip() else \"UNKNOWN\"\n",
    "        ins_curve.append(int(a == pred_choice))\n",
    "    insertion_auc = sum(ins_curve) / len(ins_curve) if ins_curve else 0.0\n",
    "\n",
    "    task_correct = (\n",
    "        int(pred_choice == gold_choice)\n",
    "        if (pred_choice in {\"A\",\"B\",\"C\",\"D\"} and gold_choice in {\"A\",\"B\",\"C\",\"D\"})\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"question_id\": question_doc.metadata.get(\"question_id\"),\n",
    "        \"question\": question,\n",
    "        \"gold_choice\": gold_choice,\n",
    "        \"pred_choice\": pred_choice,\n",
    "        \"pred_text\": pred_text,\n",
    "        \"task_correct\": task_correct,\n",
    "        \"comprehensiveness\": comprehensiveness,\n",
    "        \"sufficiency\": sufficiency,\n",
    "        \"deletion_auc\": deletion_auc,\n",
    "        \"insertion_auc\": insertion_auc,\n",
    "        \"n_sentences\": len(global_sents),\n",
    "        \"n_highlighted\": len(hl_indices),\n",
    "        \"highlight_fraction\": (len(hl_indices) / max(1, len(global_sents))),\n",
    "        \"has_evidence\": int(len(hl_indices) > 0),\n",
    "    }\n",
    "\n",
    "print(\"Evaluation core ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a01d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EVALUATION SUMMARY ===\n",
      "n_questions: 59\n",
      "task_accuracy: 0.1186\n",
      "comprehensiveness: 0.4237\n",
      "sufficiency: 0.5932\n",
      "deletion_auc: 0.4011\n",
      "insertion_auc: 0.5706\n",
      "highlight_fraction: 0.3093\n",
      "avg_sentences_per_context: 10.2542\n",
      "avg_highlighted_sentences: 2.9492\n",
      "evidence_coverage: 0.9831\n",
      "total_llm_calls: 937\n",
      "total_llm_time_s: 191.7426\n",
      "avg_llm_call_time_s: 0.2046\n",
      "wall_time_s: 228.2352\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_questions</th>\n",
       "      <th>task_accuracy</th>\n",
       "      <th>comprehensiveness</th>\n",
       "      <th>sufficiency</th>\n",
       "      <th>deletion_auc</th>\n",
       "      <th>insertion_auc</th>\n",
       "      <th>highlight_fraction</th>\n",
       "      <th>avg_sentences_per_context</th>\n",
       "      <th>avg_highlighted_sentences</th>\n",
       "      <th>evidence_coverage</th>\n",
       "      <th>total_llm_calls</th>\n",
       "      <th>total_llm_time_s</th>\n",
       "      <th>avg_llm_call_time_s</th>\n",
       "      <th>wall_time_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59</td>\n",
       "      <td>0.118644</td>\n",
       "      <td>0.423729</td>\n",
       "      <td>0.59322</td>\n",
       "      <td>0.40113</td>\n",
       "      <td>0.570621</td>\n",
       "      <td>0.309289</td>\n",
       "      <td>10.254237</td>\n",
       "      <td>2.949153</td>\n",
       "      <td>0.983051</td>\n",
       "      <td>937</td>\n",
       "      <td>191.742582</td>\n",
       "      <td>0.204635</td>\n",
       "      <td>228.235206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_questions  task_accuracy  comprehensiveness  sufficiency  deletion_auc  \\\n",
       "0           59       0.118644           0.423729      0.59322       0.40113   \n",
       "\n",
       "   insertion_auc  highlight_fraction  avg_sentences_per_context  \\\n",
       "0       0.570621            0.309289                  10.254237   \n",
       "\n",
       "   avg_highlighted_sentences  evidence_coverage  total_llm_calls  \\\n",
       "0                   2.949153           0.983051              937   \n",
       "\n",
       "   total_llm_time_s  avg_llm_call_time_s  wall_time_s  \n",
       "0        191.742582             0.204635   228.235206  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>question</th>\n",
       "      <th>gold_choice</th>\n",
       "      <th>pred_choice</th>\n",
       "      <th>pred_text</th>\n",
       "      <th>task_correct</th>\n",
       "      <th>comprehensiveness</th>\n",
       "      <th>sufficiency</th>\n",
       "      <th>deletion_auc</th>\n",
       "      <th>insertion_auc</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>n_highlighted</th>\n",
       "      <th>highlight_fraction</th>\n",
       "      <th>has_evidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7f444937-f1ae-403c-9427-34f6d5c18aa6</td>\n",
       "      <td>Which of the following agents is likely to cau...</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>Spiramycin</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6ffd899a-4d4b-4216-b8ac-6e16a4b0daa1</td>\n",
       "      <td>Myocarditis is caused bya) Pertussisb) Measles...</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>Diphtheria</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3f63787d-7816-48fe-a623-b61ba10a3001</td>\n",
       "      <td>Childhood osteopetrosis is characterized by – ...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>frontal bossing</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f7c6e673-3268-4c7a-abf2-dff2426a1ae0</td>\n",
       "      <td>The main function of Vitamin C in the body is -</td>\n",
       "      <td>C</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>Vitamin C’s function</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80a922e3-e55d-4cdc-8a90-3100c3647e99</td>\n",
       "      <td>The triad of hypertension, bradycardia and irr...</td>\n",
       "      <td>A</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>congestive heart failure</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>71453fe5-854a-4456-9012-04208d5132c2</td>\n",
       "      <td>Macrosomia is/are associated with:a) Gestation...</td>\n",
       "      <td>D</td>\n",
       "      <td>B</td>\n",
       "      <td>Maternal complications</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8409ee38-1922-4ac9-9178-ba699e33e643</td>\n",
       "      <td>Long term use of lithium causes -</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>suicidal effects</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9d0968c7-0bfa-42c5-9e1f-1cf35c40b36e</td>\n",
       "      <td>Dupuytren's contracture mvolves-</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>hand contractures</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>d4f06476-b47f-4bf7-9d32-531db1974a8e</td>\n",
       "      <td>Mechanism of action of beta-Lactam antibiotics...</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>Beta-lactam action</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>b452daac-d602-4482-a09a-6294405b6f61</td>\n",
       "      <td>Iron absorption is increased by which of the f...</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>Dietary elements</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            question_id  \\\n",
       "0  7f444937-f1ae-403c-9427-34f6d5c18aa6   \n",
       "1  6ffd899a-4d4b-4216-b8ac-6e16a4b0daa1   \n",
       "2  3f63787d-7816-48fe-a623-b61ba10a3001   \n",
       "3  f7c6e673-3268-4c7a-abf2-dff2426a1ae0   \n",
       "4  80a922e3-e55d-4cdc-8a90-3100c3647e99   \n",
       "5  71453fe5-854a-4456-9012-04208d5132c2   \n",
       "6  8409ee38-1922-4ac9-9178-ba699e33e643   \n",
       "7  9d0968c7-0bfa-42c5-9e1f-1cf35c40b36e   \n",
       "8  d4f06476-b47f-4bf7-9d32-531db1974a8e   \n",
       "9  b452daac-d602-4482-a09a-6294405b6f61   \n",
       "\n",
       "                                            question gold_choice pred_choice  \\\n",
       "0  Which of the following agents is likely to cau...           B           A   \n",
       "1  Myocarditis is caused bya) Pertussisb) Measles...           A           B   \n",
       "2  Childhood osteopetrosis is characterized by – ...           A           A   \n",
       "3    The main function of Vitamin C in the body is -           C     UNKNOWN   \n",
       "4  The triad of hypertension, bradycardia and irr...           A     UNKNOWN   \n",
       "5  Macrosomia is/are associated with:a) Gestation...           D           B   \n",
       "6                  Long term use of lithium causes -           C           A   \n",
       "7                   Dupuytren's contracture mvolves-           B           A   \n",
       "8  Mechanism of action of beta-Lactam antibiotics...           B           A   \n",
       "9  Iron absorption is increased by which of the f...           D           A   \n",
       "\n",
       "                  pred_text  task_correct  comprehensiveness  sufficiency  \\\n",
       "0                Spiramycin             0                  0            0   \n",
       "1                Diphtheria             0                  0            0   \n",
       "2           frontal bossing             1                  0            1   \n",
       "3      Vitamin C’s function             0                  1            1   \n",
       "4  congestive heart failure             0                  0            1   \n",
       "5    Maternal complications             0                  0            0   \n",
       "6          suicidal effects             0                  1            1   \n",
       "7         hand contractures             0                  0            0   \n",
       "8        Beta-lactam action             0                  0            0   \n",
       "9          Dietary elements             0                  1            0   \n",
       "\n",
       "   deletion_auc  insertion_auc  n_sentences  n_highlighted  \\\n",
       "0      0.000000       0.000000            8              3   \n",
       "1      0.666667       0.000000           11              3   \n",
       "2      0.000000       1.000000           12              3   \n",
       "3      0.666667       1.000000           13              3   \n",
       "4      0.000000       1.000000           12              3   \n",
       "5      0.666667       0.666667            4              3   \n",
       "6      1.000000       0.666667           13              3   \n",
       "7      0.000000       0.000000           15              3   \n",
       "8      0.166667       0.000000           10              3   \n",
       "9      1.000000       0.000000           13              3   \n",
       "\n",
       "   highlight_fraction  has_evidence  \n",
       "0            0.375000             1  \n",
       "1            0.272727             1  \n",
       "2            0.250000             1  \n",
       "3            0.230769             1  \n",
       "4            0.250000             1  \n",
       "5            0.750000             1  \n",
       "6            0.230769             1  \n",
       "7            0.200000             1  \n",
       "8            0.300000             1  \n",
       "9            0.230769             1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate across all MedMCQA questions (StatPearls corpus)\n",
    "\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "t0 = time.time()\n",
    "rows = [run_one(doc) for doc in documents]\n",
    "wall_time_s = time.time() - t0\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "summary = {\n",
    "    \"n_questions\": int(len(df)),\n",
    "    \"task_accuracy\": float(df[\"task_correct\"].mean()),\n",
    "    \"comprehensiveness\": float(df[\"comprehensiveness\"].mean()),\n",
    "    \"sufficiency\": float(df[\"sufficiency\"].mean()),\n",
    "    \"deletion_auc\": float(df[\"deletion_auc\"].mean()),\n",
    "    \"insertion_auc\": float(df[\"insertion_auc\"].mean()),\n",
    "    \"highlight_fraction\": float(df[\"highlight_fraction\"].mean()),\n",
    "    \"avg_sentences_per_context\": float(df[\"n_sentences\"].mean()),\n",
    "    \"avg_highlighted_sentences\": float(df[\"n_highlighted\"].mean()),\n",
    "    \"evidence_coverage\": float(df[\"has_evidence\"].mean()),\n",
    "    \"total_llm_calls\": int(LLM_CALLS),\n",
    "    \"total_llm_time_s\": float(LLM_TIME_S),\n",
    "    \"avg_llm_call_time_s\": float(LLM_TIME_S / max(1, LLM_CALLS)),\n",
    "    \"wall_time_s\": float(wall_time_s),\n",
    "}\n",
    "\n",
    "print(\"=== EVALUATION SUMMARY ===\")\n",
    "for k, v in summary.items():\n",
    "    if isinstance(v, float):\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "    else:\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "display(pd.DataFrame([summary]))\n",
    "df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
